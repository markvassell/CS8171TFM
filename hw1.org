#+title: Homework 1: Small-scale Machine Learning with Hand-Crafted Features
#+author: Toni Kazic
#+date: <2018-03-08 Thu>



* Instructions

Please put your answers right after each question.  Answer the questions
with both links to your files (*use relative paths beginning with
../../repo/s18 in this file!*) and discuss your work: your ideas, approach,
what worked, what didn't, and what you would do differently.  The
discussion should be succinct.


Please post the completed file to canvas no later than noon on
<2018-03-12 Mon> (question 1) and <2018-03-19 Mon> (question 2).
Canvas identifies your submission, so there's no need to put your name and
student number in the file (and please don't, for FERPA reasons).



* Task

The basic plan is to take some data sets, embed them in several different
ways, and then try to distinguish bot from non-bot posts in supervised
classification. 


Each person has one embedding to produce and one ML algorithm to use on all
the embeddings.


Your description and discussion of your work goes in this file, with links
to your code, embeddings, and ML results on the repo. *Please use relative
paths beginning with ../../repo/s18 in this file!* Please label each file
with the lower-cased name of the embedding or ML algorithm.


I suggest you use [[ http://scikit-learn.org/stable/modules/linear_model.html][scikit-learn]]'s implementations of the ML algorithms, but
you are free to use any implementation provided you meet the constraints of
your problem.



** Data Sets

  [[./data/bot_scoringv2.csv][Bot Scoring V2]]

  [[./data/nonbot_scoring.csv][Non Bot Scoring]]

  [[./data/train_test_data/bot_compiled_scores.csv][Bot Compiled Scores]] 

  [[./data/train_test_data/nonbot_compiled_scores.csv][Nonbot Compiled Scores]]



** 1.  Embeddings (due <2018-03-12 Mon>)

+ Code goes on the repo at s18/new_code/embeddings.
+ Results go on the repo at s18/results/embeddings.
+ Your amended version of this file goes on canvas.


Use the wdr, dissim, and leven scores for each data set and:

   + rescale the two dimensions with the smaller dynamic ranges to the
     range of the dimension having the largest dynamic range. :samika:

   + Z-score for each dimension :rui:

   + rescale all three dimensions based on each value's rank in its
     range. :mark:

   + bin each dimension into 20-ciles, then recode each dimension's value
     with its bin number. :said:

   + spectral embedding, retaining the best dimensions :aquila:

   + position of value relative to the major mode of each dimension's
     distribution :derek:

   + multi-dimensional scaling :will:


** Embedding Solution 

For my rescaling I use a simple methed to the results from 0 to 1. 
In this method 0 is the lowest number in the list and 1 is the 
greatest. The equation:
#+name: Rescaling Algo
#+BEGIN_SRC calc
x' = (x - min(x))/(max(x) - min(x))
#+END_SRC 

A view of one of my results can be found here [[./results/embeddings/rescaled_bot_scoringv2.csv][Rescaled Bot Scoring V2]]

Here is a view of my rescale function in python 3.6
#+BEGIN_SRC 
def rescale(data):
	max_val = max(data)
	min_val = min(data)
	new_data = list()
	for val in data:
		new_val = (val - min_val) / (max_val - min_val)
		new_data.append(new_val)


#+END_SRC

Here is a view of how I extracted the data from the file and rewrote it to
the rescaled file.

#+BEGIN_SRC
if 'bot_scoringv2.csv' in filename:
	try:
		count = 0
		with open(filename, 'r') as dataFile:
			for line in dataFile:
				if count != 0:
					current_data = line.split(',')
					identification.append(current_data[0])
					wdr.append(float(current_data[1]))
					dissim.append(float(current_data[2]))
					leven.append(float(current_data[3].replace('\n','')))

				count += 1

		new_wdr = rescale(wdr)
		new_dissim = rescale(dissim)
		new_leven = rescale(leven)

		with open('../Results/rescaled_bot_scoringv2.csv', 'w+') as resultFile:
			resultFile.write('uid,wdr,dissim,leven\n')
			i = 0
			while i < len(identification):
				resultFile.write(identification[i] + ',' + str(new_wdr[i]) + ',' + str(new_dissim[i]) + ',' + str(new_leven[i]) + '\n')
				i += 1

		print ("Data from %s has successfully been rescaled." % filename)

	except:
		print("Your file name may not be valid: filename - '%s'" %filename)

#+END_SRC

** 2.  ML algorithms (due <2018-03-19 Mon>)

+ Code goes on the repo at s18/new_code/ml_sm_set.
+ Results go on the repo at s18/results/ml_sm_set.
+ Your amended version of this file goes on canvas.



Please use the following ML algorithms for the all the different embeddings
of all the data sets.  Note each has two pairs of labelled data.


   + Logistic regression/Maximum Entropy (explore effects of tuning parameter
     values)  :mark:

   + Naive Bayes (choose algorithm based on distribution of the data) :rui:

   + Perceptron with one hidden layer, trained with back-propagation, with
     and without regularization :aquila:

   + affinity propagation :derek:

   + Gaussian process regression (Gaussian and radial basis function kernels) :samika:

   + SVM :sai:

   + spectral clustering :will:





* Grading Scale

This homework is worth 20 points. The grading scale is:  


| fraction correctly answered | points awarded |
|-----------------------------+----------------|
| >= 0.9                      |             20 |
| 0.8 -- 0.89                 |             17 |
| 0.7 -- 0.79                 |             14 |
| 0.6 -- 0.69                 |             11 |
| 0.5 -- 0.59                 |              8 |
| 0.4 -- 0.49                 |              5 |
| 0.3 -- 0.39                 |              3 |
| < 0.3                       |              0 |







* Scoring

This homework is worth 20 points, with each question worth 10 points.  The
scale is:


| question     | answer ok? |
|--------------+------------|
| 1            |            |
| 2            |            |
|--------------+------------|
| total score  |            |
| percentage   |            |
| total points |            |
#+TBLFM: @4$2=vsum(@2..@3)::@5$2=@4/20



